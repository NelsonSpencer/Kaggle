{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3</td>\n",
       "      <td>                           Braund, Mr. Owen Harris</td>\n",
       "      <td>   male</td>\n",
       "      <td> 22</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>        A/5 21171</td>\n",
       "      <td>  7.2500</td>\n",
       "      <td>  NaN</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 2</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td> female</td>\n",
       "      <td> 38</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>         PC 17599</td>\n",
       "      <td> 71.2833</td>\n",
       "      <td>  C85</td>\n",
       "      <td> C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 3</td>\n",
       "      <td> 1</td>\n",
       "      <td> 3</td>\n",
       "      <td>                            Heikkinen, Miss. Laina</td>\n",
       "      <td> female</td>\n",
       "      <td> 26</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> STON/O2. 3101282</td>\n",
       "      <td>  7.9250</td>\n",
       "      <td>  NaN</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 4</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td>      Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td> female</td>\n",
       "      <td> 35</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>           113803</td>\n",
       "      <td> 53.1000</td>\n",
       "      <td> C123</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 5</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3</td>\n",
       "      <td>                          Allen, Mr. William Henry</td>\n",
       "      <td>   male</td>\n",
       "      <td> 35</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>           373450</td>\n",
       "      <td>  8.0500</td>\n",
       "      <td>  NaN</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male   22      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "2                             Heikkinen, Miss. Laina  female   26      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
       "4                           Allen, Mr. William Henry    male   35      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 714.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td> 446.000000</td>\n",
       "      <td>   0.383838</td>\n",
       "      <td>   2.308642</td>\n",
       "      <td>  29.699118</td>\n",
       "      <td>   0.523008</td>\n",
       "      <td>   0.381594</td>\n",
       "      <td>  32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td> 257.353842</td>\n",
       "      <td>   0.486592</td>\n",
       "      <td>   0.836071</td>\n",
       "      <td>  14.526497</td>\n",
       "      <td>   1.102743</td>\n",
       "      <td>   0.806057</td>\n",
       "      <td>  49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   0.420000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td> 223.500000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   2.000000</td>\n",
       "      <td>  20.125000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td> 446.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   3.000000</td>\n",
       "      <td>  28.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>  14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td> 668.500000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   3.000000</td>\n",
       "      <td>  38.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>  31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td> 891.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   3.000000</td>\n",
       "      <td>  80.000000</td>\n",
       "      <td>   8.000000</td>\n",
       "      <td>   6.000000</td>\n",
       "      <td> 512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that the age count is less than the rest of the columns, which means that we are missing data** \n",
    "\n",
    "\n",
    "What should we do about this? We have a few options:\n",
    "        1. List wise deletion: Delete observations where any of the variable is missing\n",
    "        2. Pair Wise Deletion: Include all cases in data analysis in which the variables of interest are present\n",
    "        3. Mean/Mode Substitution: Replace missing values with the mean or mode of the other values in that column\n",
    "\n",
    "\n",
    "We'll use option 3 here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data[\"Age\"] = training_data[\"Age\"].fillna(training_data[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use describe again to verify our age count is accurate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "      <td> 891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td> 446.000000</td>\n",
       "      <td>   0.383838</td>\n",
       "      <td>   2.308642</td>\n",
       "      <td>  29.361582</td>\n",
       "      <td>   0.523008</td>\n",
       "      <td>   0.381594</td>\n",
       "      <td>  32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td> 257.353842</td>\n",
       "      <td>   0.486592</td>\n",
       "      <td>   0.836071</td>\n",
       "      <td>  13.019697</td>\n",
       "      <td>   1.102743</td>\n",
       "      <td>   0.806057</td>\n",
       "      <td>  49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   0.420000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td> 223.500000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   2.000000</td>\n",
       "      <td>  22.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td> 446.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   3.000000</td>\n",
       "      <td>  28.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>  14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td> 668.500000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   3.000000</td>\n",
       "      <td>  35.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   0.000000</td>\n",
       "      <td>  31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td> 891.000000</td>\n",
       "      <td>   1.000000</td>\n",
       "      <td>   3.000000</td>\n",
       "      <td>  80.000000</td>\n",
       "      <td>   8.000000</td>\n",
       "      <td>   6.000000</td>\n",
       "      <td> 512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
       "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric columns above now look ready to analyze, but what about the other remaining columns in the data? For these we need to convert them to number equivalents for our model to properly analyze them. We will not be using the ticket and name columns because those most likely will not have a impact on our model.\n",
    "\n",
    "Let's first start with sex, we first need to see how many unique values are in the column and then come up with a numbering system to replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.Sex.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the diffent sexes listed, we can replace them. It's easiest just to use 0 and 1 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data.loc[training_data[\"Sex\"] == \"male\", \"Sex\"] = 0 #Use .loc for index search\n",
    "training_data.loc[training_data[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3</td>\n",
       "      <td>                           Braund, Mr. Owen Harris</td>\n",
       "      <td> 0</td>\n",
       "      <td> 22</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>        A/5 21171</td>\n",
       "      <td>  7.2500</td>\n",
       "      <td>  NaN</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 2</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td> 1</td>\n",
       "      <td> 38</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>         PC 17599</td>\n",
       "      <td> 71.2833</td>\n",
       "      <td>  C85</td>\n",
       "      <td> C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 3</td>\n",
       "      <td> 1</td>\n",
       "      <td> 3</td>\n",
       "      <td>                            Heikkinen, Miss. Laina</td>\n",
       "      <td> 1</td>\n",
       "      <td> 26</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> STON/O2. 3101282</td>\n",
       "      <td>  7.9250</td>\n",
       "      <td>  NaN</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 4</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td>      Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td> 1</td>\n",
       "      <td> 35</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>           113803</td>\n",
       "      <td> 53.1000</td>\n",
       "      <td> C123</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 5</td>\n",
       "      <td> 0</td>\n",
       "      <td> 3</td>\n",
       "      <td>                          Allen, Mr. William Henry</td>\n",
       "      <td> 0</td>\n",
       "      <td> 35</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>           373450</td>\n",
       "      <td>  8.0500</td>\n",
       "      <td>  NaN</td>\n",
       "      <td> S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name Sex  Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris   0   22      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...   1   38      1      0   \n",
       "2                             Heikkinen, Miss. Laina   1   26      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   1   35      1      0   \n",
       "4                           Allen, Mr. William Henry   0   35      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171   7.2500   NaN        S  \n",
       "1          PC 17599  71.2833   C85        C  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3            113803  53.1000  C123        S  \n",
       "4            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing to the embarked column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See if any of the values in the column are missing\n",
    "training_data['Embarked'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since the majority are S we'll fill in the blanks with S\n",
    "training_data[\"Embarked\"] = training_data[\"Embarked\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Double check to be safe\n",
    "training_data['Embarked'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data.loc[training_data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "training_data.loc[training_data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "training_data.loc[training_data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a helper from sklearn to split the data up into cross validation folds, and then train an algorithm for each fold, and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The columns we want to use as our predictors\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "\n",
    "# Generate cross validation folds for data. Here we are making 3 diffierent folds.  \n",
    "# It returns the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run it.\n",
    "kf = KFold(training_data.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (training_data[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = training_data[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(training_data[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluating Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define some sort of error metric in order to see how accurate the model is. From the Kaggle competition description, the error metric is percentage of correct predictions. This generally involves finding the number of values in predictions that are the exact same as their counterparts in data[\"Survived\"], and then dividing by the total number of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:8: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "accuracy = sum(predictions[predictions == training_data[\"Survived\"]]) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833894500561167"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy isn't great, let's use logistic regression to map our value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.\n",
    "scores = cross_validation.cross_val_score(alg, training_data[predictors], training_data[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't great, but let's go ahead anyway so that we can get a first submission in. (We'll come back later and improve the algorithm). In order to get our submission, we have to read in the test data set from Kaggle and perform the same steps as we did on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data[\"Age\"] = test_data[\"Age\"].fillna(training_data[\"Age\"].median())\n",
    "test_data[\"Fare\"] = test_data[\"Fare\"].fillna(test_data[\"Fare\"].median())\n",
    "test_data.loc[test_data[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "test_data.loc[test_data[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "test_data[\"Embarked\"] = test_data[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "test_data.loc[test_data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "test_data.loc[test_data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "test_data.loc[test_data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate our submission we have to train an algorithm on the training data. Then, we make predictions on the test set. Finally, we'll generate a csv file with the predictions and passenger ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(training_data[predictors], training_data[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(test_data[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 892</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 893</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 894</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 895</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 896</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export submission to csv to submit to kaggle\n",
    "submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Improving first submission\n",
    "\n",
    "In general there a few ways to improve accuracy:\n",
    "        1. Use a better algorithm  \n",
    "        2. Generate better features\n",
    "        3. Combine multiple algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a new algorithm - Random Forests. More details on random forests can be found [here.](http://www.nelsonspencer.com/blog/2015/2/15/machine-learning-supervised-learning-pt-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800224466891\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where \n",
    "# a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "# Compute the accuracy score for all the cross validation folds\n",
    "scores = cross_validation.cross_val_score(alg, training_data[predictors], training_data[\"Survived\"], cv=3)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tuning a bit\n",
    "\n",
    "Good news! We've increased our accuracy, but let's think about how we can tweak it to get an even higher score. As you might have guessed, we can increase the number of trees (n_estimators) but because of the fact that we're averaging many predictions made on different subsets of the data, having more trees will only increase accuracy up to a point.\n",
    "\n",
    "In addition to tweaking the number of trees, we can look at the min_samples_split and min_samples_leaf samples in order to reduce overfitting. You never want the trees to match the data perfectly because then it incompasses all of the quirks in the data and doesn't help when we present the algorithm with new data.\n",
    "\n",
    "So we'll increase the number trees, splits, and sample leafs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, training_data[predictors], training_data[\"Survived\"], cv=3)\n",
    "\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generating New Features\n",
    "\n",
    "Awesome, another higher accuracy score!\n",
    "\n",
    "In addition to the tweaks we made above, sometimes (depending on the data), you can try and generate new features to test on. For example with our data we could look at:\n",
    "\n",
    "    1. The length of the name which could pertain to how rich a person was, and therefore their position on the ship\n",
    "    2. The total number of people in a family (SibSp + Parch)\n",
    "    \n",
    "    \n",
    "To generate these new features we'll have to manipulate our data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "training_data[\"FamilySize\"] = training_data[\"SibSp\"] + training_data[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "training_data[\"NameLength\"] = training_data[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature we can look at is the titles of each of the passengers using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A function to get the title from a name\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = training_data[\"Name\"].apply(get_title)\n",
    "print pd.value_counts(titles)\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print pd.value_counts(titles)\n",
    "\n",
    "# Add in the title column.\n",
    "training_data[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a feature indicating which family people are in. Because survival was likely highly dependent on your family and the people around you, this has a good chance at being a good feature.\n",
    "\n",
    "To get this, we'll concatenate someone's last name with FamilySize to get a unique family id. We'll then be able to assign a code to each person based on their family id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = training_data.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[training_data[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print pd.value_counts(family_ids)\n",
    "\n",
    "training_data[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Choosing Best Features\n",
    "\n",
    "Now that we have more features to use in our algorithm, we have to choose which ones are the best. Sklearn has another helpful function that does this (SelectKBest), it eseentially goes column by column and figures out which columns correlate most closely with what we're trying to predict (Survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAEpCAYAAAC3ChhmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSGBHRDQigR0R0Yj/D112ZZHlG1eLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dfcc0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811447811448\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(training_data[predictors], training_data[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "# Compute the accuracy score for all the cross validation folds.  \n",
    "scores = cross_validation.cross_val_score(alg, training_data[predictors], training_data[\"Survived\"], cv=3)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method that builds on decision trees is a gradient boosting classifier. Boosting involves training decision trees one after another, and feeding the errors from one tree into the next tree. So each tree is building on all the other trees that came before it. This can lead to overfitting if we build too many trees, though. As you get above 100 trees or so, it's very easy to overfit the dataset. As our dataset is small, we'll limit the tree count to 25.\n",
    "\n",
    "Another way to limit overfitting is to limit the depth to which each tree in the gradient boosting process can be built. We'll limit the tree depth to 3 to avoid overfitting.\n",
    "\n",
    "We'll try boosting instead of our random forest approach and see if we can improve our accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ensembling \n",
    "\n",
    "One thing we can do to improve the accuracy of our predictions is to ensemble different classifiers. Ensembling means that we generate predictions using information from a set of classifiers, instead of just one. In practice, this means that we average their predictions.\n",
    "\n",
    "Generally, the more diverse the models we ensemble, the higher our accuracy will be. Diversity means that the models generate their results from different columns, or use a very different method to generate predictions. Ensembling a random forest classifier with a decision tree probably won't work extremely well, because they are very similar. On the other hand, ensembling a linear regression with a random forest can work very well.\n",
    "\n",
    "One caveat with ensembling is that the classifiers we use have to be about the same in terms of accuracy. Ensembling one classifier that is much worse than another probably will make the final result worse.\n",
    "\n",
    "In this case, we'll ensemble logistic regression trained on the most linear predictors (the ones that have a linear ordering, and some correlation to Survived), and a gradient boosted tree trained on all of the predictors.\n",
    "\n",
    "We'll keep things simple when we ensemble -- we'll average the raw probabilities (from 0 to 1) that we get from our classifiers, and then assume that anything above .5 maps to one, and anything below or equal to .5 maps to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:34: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(training_data.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = training_data[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(training_data[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(training_data[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == training_data[\"Survived\"]]) / len(predictions)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Making Second Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take everything we've done on the training set and apply it to the test set. Remember we:\n",
    "\n",
    "        1. Generate the NameLength column, which is how long the name is\n",
    "        2. Generate the FamilySize column, showing how large a family is\n",
    "        3. Add in the Title column, keeping the same mapping that we had before\n",
    "        4. Add in a FamilyId column, keeping the ids consistent across the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First, we'll add titles to the test set.\n",
    "titles = test_data[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "test_data[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print pd.value_counts(test_data[\"Title\"])\n",
    "\n",
    "# Now, we add the family size column.\n",
    "test_data[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "#print family_id_mapping\n",
    "\n",
    "family_ids = test_data.apply(get_family_id, axis=1)\n",
    "family_ids[test_data[\"FamilySize\"] < 3] = -1\n",
    "test_data[\"FamilyId\"] = family_ids\n",
    "test_data[\"NameLength\"] = test_data[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Predict on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(training_data[predictors], training_data[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(test_data[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Future ideas to keep exploring:\n",
    "\n",
    "Feature engineering:\n",
    "\n",
    "        Try using features related to the cabins.\n",
    "        See if any family size features might help -- do the number of women in a family make the whole family more    likely to survive?\n",
    "        Does the national origin of the passenger's name have anything to do with survival?\n",
    "\n",
    "The algorithm side:\n",
    "\n",
    "        Try the random forest classifier in the ensemble.\n",
    "        A support vector machine might work well with this data.\n",
    "        We could try neural networks.\n",
    "        Boosting with a different base classifier might work better.\n",
    "\n",
    "Ensembling methods:\n",
    "\n",
    "    Could majority voting be a better ensembling method than averaging probabilities?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
